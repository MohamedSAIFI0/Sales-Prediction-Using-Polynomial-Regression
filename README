# Polynomial Regression from Scratch

A comprehensive implementation of polynomial regression using gradient descent, built from scratch with NumPy. This project demonstrates model selection, regularization techniques, and performance evaluation using the Advertising dataset.

## 📋 Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Dataset](#dataset)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Model Selection](#model-selection)
- [Results](#results)
- [Mathematical Foundation](#mathematical-foundation)
- [Dependencies](#dependencies)
- [Contributing](#contributing)
- [License](#license)

## 🎯 Overview

This project implements polynomial regression from scratch to predict sales based on advertising expenditure across different media channels (TV, Radio, Newspaper). The implementation includes:

- Feature normalization
- Polynomial feature engineering (degree 2)
- Gradient descent optimization
- Multiple model selection criteria (MSE, BIC, Cross-validation)
- Performance evaluation using R² score

## ✨ Features

- **Custom Implementation**: Built from scratch without using scikit-learn's regression models
- **Polynomial Features**: Automatically generates polynomial terms (degree 2) including:
  - Original features: X₁, X₂, X₃
  - Squared terms: X₁², X₂², X₃²
  - Interaction terms: X₁X₂, X₁X₃, X₂X₃
- **Feature Normalization**: Standardizes features (mean=0, std=1)
- **Gradient Descent**: Optimized parameter learning with configurable learning rate
- **Model Selection**: Compares multiple criteria:
  - Test MSE (Mean Squared Error)
  - BIC (Bayesian Information Criterion)
  - Cross-validation scores
- **Visualization**: Plots cost function evolution and model selection metrics

## 📊 Dataset

**Advertising.csv** contains marketing data with the following features:

| Feature | Description |
|---------|-------------|
| TV | Advertising budget spent on TV (in thousands) |
| Radio | Advertising budget spent on Radio (in thousands) |
| Newspaper | Advertising budget spent on Newspaper (in thousands) |
| Sales | Product sales (in thousands of units) - **Target Variable** |

- **Total samples**: 200
- **Train/Test split**: 80/20

## 🚀 Installation

### Prerequisites

- Python 3.7 or higher
- pip package manager

### Step-by-Step Installation

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/polynomial-regression-scratch.git
cd polynomial-regression-scratch
```

2. **Create a virtual environment (recommended):**
```bash
# On Windows
python -m venv venv
venv\Scripts\activate

# On macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

3. **Install required packages:**
```bash
pip install -r requirements.txt
```

Or install manually:
```bash
pip install numpy pandas matplotlib scikit-learn jupyter
```

## 📁 Project Structure
```
polynomial-regression-scratch/
│
├── RegressionPolynomiale_From_Scratch.ipynb  # Main Jupyter notebook
├── Advertising.csv                           # Dataset
├── README.md                                 # Project documentation
├── requirements.txt                          # Python dependencies
├── LICENSE                                   # License file
└── images/                                   # Visualization outputs
    ├── cost_evolution.png
    └── model_selection.png
```

## 💻 Usage

### Running the Notebook

1. **Launch Jupyter Notebook:**
```bash
jupyter notebook
```

2. **Open** `RegressionPolynomiale_From_Scratch.ipynb`

3. **Run all cells** sequentially (Cell → Run All)

### Key Code Sections

#### 1. Data Loading and Preprocessing
```python
import pandas as pd
import numpy as np

dataset = pd.read_csv("Advertising.csv")
X = dataset[['TV','Radio','Newspaper']].values
y = dataset['Sales'].values
```

#### 2. Feature Normalization
```python
def normalize_features(X):
    """Normalize features (Mean = 0, Standard Deviation = 1)."""
    mu = np.mean(X, axis=0)
    sigma = np.std(X, axis=0)
    sigma[sigma == 0] = 1  # Prevent division by zero
    X_norm = (X - mu) / sigma
    return X_norm, mu, sigma

X_train_norm, mu, sigma = normalize_features(X_train)
```

#### 3. Polynomial Feature Generation
```python
def feature_poly(X_norm):
    """
    Create polynomial terms of degree 2 for 3 features:
    [X1, X2, X3, X1², X2², X3², X1*X2, X1*X3, X2*X3, 1 (bias)]
    """
    X1, X2, X3 = X_norm[:, 0:1], X_norm[:, 1:2], X_norm[:, 2:3]
    
    # Polynomial terms of degree 2
    poly_terms = np.hstack((
        X1**2, X2**2, X3**2,          # Squares
        X1 * X2, X1 * X3, X2 * X3     # Interactions
    ))
    
    # Combine original and polynomial terms
    X_poly = np.hstack((X_norm, poly_terms))
    
    # Add bias term (column of 1s)
    X_poly = np.hstack((X_poly, np.ones((X_poly.shape[0], 1)))) 
    return X_poly

X_poly_train = feature_poly(X_train_norm)
```

#### 4. Training with Gradient Descent
```python
learning_rate = 0.01
n_iterations = 20000

theta_final, cost_history = gradient_descent(
    X_poly_train, 
    y_train, 
    theta, 
    learning_rate, 
    n_iterations
)
```

## 🔍 Model Selection

The project evaluates polynomial regression models from degree 1 to 9 using three criteria:

### 1. Test MSE (Mean Squared Error)
- Measures prediction error on held-out test set
- Formula: `MSE = (1/n) Σ(y_pred - y_true)²`
- Lower is better

### 2. BIC (Bayesian Information Criterion)
```
BIC = n·log(MSE) + k·log(n)
```
Where:
- n = number of samples
- k = number of parameters
- Penalizes model complexity
- Balances fit and simplicity

### 3. Cross-Validation
- 5-fold cross-validation
- Provides robust performance estimate
- Reduces overfitting risk

### Results Summary

According to all three criteria, **degree 2** is optimal:
- ✅ **Degrees 1-8**: Acceptable
- ❌ **Degree 9**: Overfitting (avoid absolutely)

## 📈 Results

### Final Model Performance
```
Number of features: 10 (including bias)
--------------------------------------------------
Initial Cost (before training): 106.3155
Final Cost (after 20,000 iterations): 0.9539
Performance (R²) on Test Set: 0.9533
--------------------------------------------------
```

### Coefficients (θ)
```python
Coefficients finals (Theta):
[
  4.4837,   # TV
  1.5053,   # Radio
  0.0633,   # Newspaper
 -0.7493,   # TV²
  0.3155,   # Radio²
  0.0350,   # Newspaper²
  0.5198,   # TV × Radio
 -0.0436,   # TV × Newspaper
  0.0493,   # Radio × Newspaper
 15.6832    # Bias
]
```

### Interpretation

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **R² Score** | 0.9533 | Model explains 95.33% of variance in sales |
| **Cost Reduction** | 106.32 → 0.95 | Strong convergence achieved |
| **TV Impact** | θ₁ = 4.48 | Strongest positive impact on sales |
| **Radio Impact** | θ₂ = 1.51 | Significant positive impact |
| **Newspaper Impact** | θ₃ = 0.06 | Minimal impact on sales |

### Key Insights

1. **TV Advertising**: Most effective channel (coefficient = 4.48)
2. **Radio Advertising**: Second most effective (coefficient = 1.51)
3. **Newspaper Advertising**: Negligible impact (coefficient = 0.06)
4. **Synergy Effect**: TV × Radio interaction is positive (0.52), suggesting combined campaigns work well
5. **Diminishing Returns**: Negative TV² coefficient (-0.75) suggests diminishing returns at high budgets

## 🧮 Mathematical Foundation

### Cost Function (MSE)
```
J(θ) = (1/2m) Σ(h_θ(x^(i)) - y^(i))²
```

### Cost Function with L2 Regularization (Ridge)
```
J(θ) = (1/2m) Σ(h_θ(x^(i)) - y^(i))² + (λ/2m) Σθ_j²
```

### Gradient Descent Update Rule
```
θ_j := θ_j - α(1/m) Σ(h_θ(x^(i)) - y^(i))x_j^(i)
```

Where:
- m = number of training examples
- α = learning rate
- h_θ(x) = θ^T x (hypothesis function)
- λ = regularization parameter

### R² Score (Coefficient of Determination)
```
R² = 1 - (SS_res / SS_tot)

where:
SS_res = Σ(y_true - y_pred)²  # Residual sum of squares
SS_tot = Σ(y_true - ȳ)²       # Total sum of squares
```

## 📦 Dependencies

Create a `requirements.txt` file with:
```
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
scikit-learn>=0.24.0
jupyter>=1.0.0
```

Install all dependencies:
```bash
pip install -r requirements.txt
```

## 🔧 Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate (α) | 0.01 | Step size for gradient descent |
| Iterations | 20,000 | Number of training iterations |
| Polynomial Degree | 2 | Degree of polynomial features |
| Train/Test Split | 80/20 | Data split ratio |
| Random State | 42 | For reproducibility |
| CV Folds | 5 | Cross-validation folds |

### Tuning Tips

- **Learning Rate Too High**: Cost oscillates or diverges
- **Learning Rate Too Low**: Slow convergence
- **Optimal Range**: 0.001 - 0.1 (dataset dependent)
- **Check Convergence**: Plot cost vs iterations

## 📊 Visualizations

The notebook generates two key visualizations:

### 1. Model Selection Comparison
- X-axis: Polynomial degree (1-9)
- Y-axis: Score (lower is better)
- Lines: MSE (test), BIC, Cross-validation
- **Insight**: All metrics agree on degree 2

### 2. Cost Function Evolution
- X-axis: Iterations (0-20,000)
- Y-axis: Cost J(θ)
- **Shows**: Smooth convergence from 106.32 to 0.95
- **Validates**: Learning rate is appropriate

## 🎓 Learning Outcomes

By studying this project, you will learn:

1. ✅ How to implement polynomial regression from scratch
2. ✅ Feature engineering techniques
3. ✅ Gradient descent optimization
4. ✅ Model selection strategies
5. ✅ Bias-variance tradeoff
6. ✅ Regularization concepts
7. ✅ Performance evaluation metrics

## 🤝 Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Contribution Ideas

- [ ] Implement Ridge/Lasso regularization
- [ ] Add polynomial degrees 3-5
- [ ] Create interactive visualizations
- [ ] Add more datasets
- [ ] Implement mini-batch gradient descent
- [ ] Add early stopping mechanism

## 📝 License

This project is open source and available under the [MIT License](LICENSE).
```
MIT License

Copyright (c) 2024 [Your Name]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## 👤 Author

**Mohamed SAIFI**
- Email: saifimsc@gmail.com

## 🙏 Acknowledgments

- Dataset: [Advertising.csv](https://www.kaggle.com/datasets/ashydv/advertising-dataset) from Kaggle


**Note**: This is an educational project demonstrating polynomial regression implementation. For production use, consider using optimized libraries like scikit-learn.

**Star ⭐ this repository if you found it helpful!**

---

**Last Updated**: 30-10-2025